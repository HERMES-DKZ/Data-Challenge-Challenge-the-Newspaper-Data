# Challenge the Newspaper Data

Herzlich willkommen zu der ersten Data-Challenge des Datenkompetenzzentrums HERMES. Dieses Repo dient lediglich als Zusatz zu der Challenge und die Nutzung der hier zur Verfügung gestellten Skripte ist für die Teilnahme nicht zwingend notwendig. Es sind unterschiedliche Skripte zu finden, welche alle in Python geschrieben sind und unterschiedliche Nutzen erfüllen sollen. Das erste Skript, **data-extraction.py** soll die Nachvollzeihbarkeit und Transparenz der Datenbereitstellung ermöglichen. Wir würden euch trotzdem bitten, die von uns [Bereitgestellten Daten](https://hessenbox.uni-marburg.de/getlink/fi5WMibFaZX2ueh4xBvqwM/Datensatz) herunterzuladen, da sich die Daten Online regelmäßig ändern und wir garantieren möchten, das jede*r die selbe Datengrundlage nutzt. **Image-extraction.py** wiederum, soll Teilnehmenden, die mit den Scans der Zeitungen arbeiten möchten, eine einfache Möglichkeit bieten, diese herunterzuladen, da wir diese, aufgrund ihrer Größe, nicht direkt zur Verüfung stellen können. Zu guter letzt findet sich noch das Jupyter Notebook example.ipynb, welches denjenigen, die noch wenig oder keine Erfahrung mit einer solchen explorativen Analyse haben, den Einstieg erleichern soll.
Die genaue Beschreibung der Challenge findet ihr [hier](https://hermes-hub.de/formate/challenges/challenges-ausschreibungen/challenge24_1.html). 


## Data Extraction

Das Skript für das Herunterladen der Daten findet ihr in diesem Repo mit dem Namen data-extraction.py. Dazu wurde zunächst das Python Package [ddbapi](https://pypi.org/project/ddbapi/) von Karl Krägelin verwendet, welches einen Wrapper für die API des deutschen Zeitungsportal bereitstellt und damit die Nutzung dessen vereinfacht. Eine detallierte Beschreibung zu dem Wrapper findet ihr [hier](https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151). Dies ist auch die erste Methode, die im Skript zu finden ist. Sie vereinfacht schlicht das Herunterladen von Zeitungen zwischen zwei Jahren,. Der neuen Methode "article_extractor" muss damit nur noch ein Dictionary bestehend aus Sprache, Start- und Enddatum gegeben werden, und gibt anschließend ein Pandas Dataframe mit den Zeitschriften zurück, die zwischen diesen beiden Daten liegen. Das Dataframe besteht aus den Spaltenbezeichnungen page_id, pagenumer, paper_title, provider_ddb_id, provider, zdb_id, publication_date, place_of_distribution, language, thumbnail, pagefulltext, pagename, preview_reference und plainpagefulltext. Die zweite Funktion erstellt ledigleich eine Liste aus Jahreszahlen. Dafür nimmt sie zwei Jahre entgegen und gibt eine Liste aller Jahreszahlen zurück, die dazwischen liegen. Nach den beiden Funktionsdefinitionen wird nur noch ein neues Verzeichnis für die Daten erstellt und darum wiederum ein Verzeichnis pro Jahr. Um die Dataframes nicht zu groß zu gestalten, wird jedes Jahr in drei Parts geteilt, die Dictionaries für den Funktionsaufruf zum Herunterladen definiert und dann die Funktion article_extractor aufgerufen. Im letzten Schritt werden die Dataframes dann in Pickled-Dataframes lokal gespeichert.



## Image Extraction

Neben den Metadaten und den OCR-gescannten Texten, bietet die DDB auch die Möglichkeit Bilder der Zeitschriften zu betrachten und zu nutzen. Diese sind allerdings nicht direkter Teil der API, weswegen dafür ein wenig mehr Code und Improvisation nötig ist. Wenn eure Challenge Idee in Richtung OCR-Scans gehen soll, könnt ihr dieses Skript nutzen, um die Scans der Zeitschriften herunterzuladen. Da die bereitgestellten Data-Frames bereits 159 GB groß sind, konnten wir die Bilder leider nicht direkt bereitstellen. Würde man die Bilder aller von uns in der Challenge genutzten Zeitungen nutzen wollen, wäre die Größe dieser Dateien ca 2 TB. Ihr müsst für die Teilnahme nicht alle Zeitungen des Datensatzes nutzen, sondern lediglich so viele, wie für eure Aufgabe nötig sind.
Um die Bilder der Zeitungen herunterladen zu können, benötigt ihr das hier bereitgestellte Skript **image-extraction.py** und die von uns Bereitgestellten [Daten](https://hessenbox.uni-marburg.de/getlink/fi5WMibFaZX2ueh4xBvqwM/Datensatz). Das Skript geht von der selben Ordnerstruktur aus, die dieses Repo ebenfalls hat. Kopiert also alle Zeitungsdaten in den Ordner "newspapers". Überschreibt die darin zu findene Datei, da diese nur zu Beispielzwecken enthalten ist und nur die ersten 1000 Einträge der eigentlichen Datei enthält.
Im Skript selbst werden dann zunächst, nach dem Import der benötigten Bibliotheken, Variablen definiert. Durch das Jahr und den Part wird genau eine Datei definiert von der die zugehörigen Bilder heruntergeladen werden. Der Ordner dazu, wird durch die Variable download_dir definiert. Für das Herunterladen der Bilder, benötigt ihr allerdings zunächst einen API-Key von der Deutschen Digitalen Bibliothek und damit verbunden einen Account. Einen Account erstellen und anschließend kostenlos einen Key beantragen könnt ihr [hier](https://www.deutsche-digitale-bibliothek.de/?doLogin=true&referrer=%2Fuser%2Fapikey). Diesen Key müsst ihr dann noch der Variable "api-key" zuweisen.

Im Weiteren wird so vorgegangen, dass zu jedem Eintrag aus den heruntergeladenen Metadaten eine page_id gezogen wird. Diese wird  zugeschnitten, da die ersten 32 Stellen dieser, die ID einer Ausgabe darstellen. Zuvor stand diese ID für die individuelle Seite innerhalb einer Ausgabe. Mit dieser ID wird dann über die API der DDB ein XML-File geladen, welche alle Informationen zu der Ausgabe enthält, auch Links zu den gescannten Seiten. Diese gescannten Seiten gibt es allerdings in verschiedenen Größen. Da die Scans und die hier genutzte XML nicht von der DDB, sondern von den Haltern der Daten zur Verfügung gestellt werden, gibt es keine Einheitlichkeit der Bezeichnungen, weswegen noch geprüft wird, welches Bild das größte ist, um dies dann herunterzuladen.



## Beispiel

Um den Einsteig für weniger erfahrende Forschende zu erleichtern, findet sich in diesem Repo außerdem noch ein Jupyter Notebook, "example.ipynb welches die ersten Schritte einer möglichen Analyse beschreibt und zum experimentieren einlädt. Erfahrende Forschende können dies ebenfalls als Ausgangspunkt oder zum Erhalt eines erstes Eindrucks der Daten nutzen. Um dieses Skript zu nutzen benötigt ihr, neben Python, Jupyter Notebook. Dies könnt ihr entweder mit dem Jupyter Lab nutzen oder einem Editor eurer Wahl. Eine Anleitung für die Installation von Jupyter Lab findet ihr [hier](https://jupyter.org/install). Um das Ganze zu nutzen klont am besten das Repo und öffnet es in dem Editor eurer Wahl. Ihr findet hier im Ordner "newspapers" eine Beispieldatei. Diese besteht nur aus den ersten 1000 Einträgen des Parts, da GitHub größere Dateien nicht zulässt. Wenn ihr mit den richtigen Dateien arbeiten möchtet, kopiert sie einfach in den Ordner und gebt diese im Skript an. Eine genaue Beschreibung des Codes findet ihr im Notebook selbst.
