# Challenge the Newspaper Data

Herzlich willkommen der ersten Data-Challenge des Datenkompetenzzentrums HERMES. In diesem Repo findet ihr alle Skripte, die für die Bereitstellung der Zeitungsdaten genutzt wurden, sowieso eine kurze Erklärung zu diesen, damit die Nachnutzung und Erweiterung dieser erleichtert wird. Eine sehr detallierte Beschreibung der Challenge findet ihr [hier](https://hermes-hub.de/formate/challenges/challenges-ausschreibungen/challenge24_1.html). Die Bereitstellung des Skripts data-extraction.py dient lediglich nur zur Nachvollziehbarkeit des Herunterladens. Wir würden euch bitten, die von uns [Bereitgestellten Daten](https://hessenbox.uni-marburg.de/getlink/fi5WMibFaZX2ueh4xBvqwM/Datensatz) herunterzuladen, da sich die Daten Online regelmäßig ändern und wir garantieren möchten, das jede*r die selbe Datengrundlage nutzt.



## Data Extraction

Das Skript für das Herunterladen der Daten findet ihr in diesem Repo mit dem Namen data-extraction.py. Dazu wurde zunächst das Python Package [ddbapi](https://pypi.org/project/ddbapi/) von Karl Krägelin verwendet, welches einen Wrapper für die API des deutschen Zeitungsportal bereitstellt und damit die Nutzung dessen vereinfacht. Eine detallierte Beschreibung zu dem Wrapper findet ihr [hier](https://deepnote.com/app/karl-kragelin-b83c/Zeitungsportal-API-d9224dda-8e26-4b35-a6d7-40e9507b1151). Dies ist auch die erste Methode, die im Skript zu finden ist. Sie vereinfacht schlicht das Herunterladen von Zeitungen zwischen zwei Jahren,. Der neuen Methode "article_extractor" muss damit nur noch ein Dictionary bestehend aus Sprache, Start- und Enddatum gegeben werden, und gibt anschließend ein Pandas Dataframe mit den Zeitschriften zurück, die zwischen diesen beiden Daten liegen. Das Dataframe besteht aus den Spaltenbezeichnungen page_id, pagenumer, paper_title, provider_ddb_id, provider, zdb_id, publication_date, place_of_distribution, language, thumbnail, pagefulltext, pagename, preview_reference und plainpagefulltext. Die zweite Funktion erstellt ledigleich eine Liste aus Jahreszahlen. Dafür nimmt sie zwei Jahre entgegen und gibt eine Liste aller Jahreszahlen zurück, die dazwischen liegen. Nach den beiden Funktionsdefinitionen wird nur noch ein neues Verzeichnis für die Daten erstellt und darum wiederum ein Verzeichnis pro Jahr. Um die Dataframes nicht zu groß zu gestalten, wird jedes Jahr in drei Parts geteilt, die Dictionsaries für den Funktionsaufruf zum Herunterladen definiert und dann die Funktion article_extractor aufgerufen. Im letzten Schritt werden die Dataframes dann in Pickled-Dataframes Lokal gespeichert.



## Image Extraction

Neben den Metadaten und den OCR-gescannten Texten, bietet die DDB auch die Möglichkeit Bilder der Zeitschriften zu betrachten und zu nutzen. Diese sind allerdings nicht direkter Teil der API, weswegen dafür ein wenig mehr Code und Improvisation nötig ist. Wenn eure Challenge Idee in Richtung OCR-Scans gehen soll, könnt ihr dieses Skript nutzen, um die Scans der Zeitschriften herunterzuladen. Da die bereitgestellten Data-Frames bereits 159 GB groß sind, konnten wir diese leider nicht herunterladen und bereitstellen.
Für das Herunterladen der Bilder, benötigt ihr zunächst einen API-Key von der Deutschen Digitalen Bibliothek und damit verbunden einen Account. Einen Account erstellen und anschließend kostenlos einen Key beantragen könnt ihr [hier](https://www.deutsche-digitale-bibliothek.de/?doLogin=true&referrer=%2Fuser%2Fapikey). Diesen Key müsst ihr dann noch der Variable "api-key" zuweisen.
